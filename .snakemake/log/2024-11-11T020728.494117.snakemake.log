host: login01
Building DAG of jobs...
Using shell: /usr/bin/bash
Provided cores: 80
Rules claiming more threads will be scaled down.
Job stats:
job       count
------  -------
filter        2
total         2

Select jobs to execute...
Execute 2 jobs...

[Mon Nov 11 02:07:28 2024]
localrule filter:
    input: gdc_sample_sheet.tsv
    output: sample_files/primary_tumor.tsv
    jobid: 0
    reason: Missing output files: sample_files/primary_tumor.tsv
    wildcards: file_name=primary_tumor
    resources: tmpdir=/tmp


[Mon Nov 11 02:07:28 2024]
localrule filter:
    input: gdc_sample_sheet.tsv
    output: sample_files/solid_tissue_normal.tsv
    jobid: 1
    reason: Missing output files: sample_files/solid_tissue_normal.tsv
    wildcards: file_name=solid_tissue_normal
    resources: tmpdir=/tmp

[Mon Nov 11 02:07:35 2024]
Error in rule filter:
    jobid: 1
    input: gdc_sample_sheet.tsv
    output: sample_files/solid_tissue_normal.tsv

[Mon Nov 11 02:07:35 2024]
Error in rule filter:
    jobid: 0
    input: gdc_sample_sheet.tsv
    output: sample_files/primary_tumor.tsv

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: .snakemake/log/2024-11-11T020728.494117.snakemake.log
WorkflowError:
At least one job did not complete successfully.
